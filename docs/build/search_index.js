var documenterSearchIndex = {"docs":
[{"location":"public/#Public-Interface-1","page":"Public Interface","title":"Public Interface","text":"","category":"section"},{"location":"public/#","page":"Public Interface","title":"Public Interface","text":"CurrentModule = CoTETE","category":"page"},{"location":"public/#","page":"Public Interface","title":"Public Interface","text":"estimate_TE_from_event_times","category":"page"},{"location":"public/#CoTETE.estimate_TE_from_event_times","page":"Public Interface","title":"CoTETE.estimate_TE_from_event_times","text":"estimate_TE_from_event_times(\n  parameters::CoTETEParameters,\n  target_events::Array{<:AbstractFloat},\n  source_events::Array{<:AbstractFloat};\n  conditioning_events::Array{<:AbstractFloat} = Float32[],\n)\n\nEstimate the TE from lists of raw event times.\n\ninfo: Single conditioning process\nNote that although the framework developed in our paper   considers an arbitrary number of extra   conditioning processes, at present the framework can only handle a single such process.   This will change in future releases.\n\nExamples\n\nThis example demonstrates estimating the TE between uncoupled homogeneous Poisson processes. This is covered in section II A of our paper. We first create the source and target processes, each with 10 000 events and with rate 1, before running the estimator.\n\njulia> source = sort(1e4*rand(Int(1e4)));\n\njulia> target = sort(1e4*rand(Int(1e4)));\n\njulia> parameters = CoTETE.CoTETEParameters(l_x = 1, l_y = 1);\n\njulia> TE = CoTETE.estimate_TE_from_event_times(parameters, target, source)\n0.0\n\njulia> abs(TE - 0) < 0.05 # For Doctesting purposes\ntrue\n\n\nWe can also try increasing the length of the target and source history embeddings\n\njulia> parameters = CoTETE.CoTETEParameters(l_x = 3, l_y = 3);\n\njulia> TE = CoTETE.estimate_TE_from_event_times(parameters, target, source)\n0.0\n\njulia> abs(TE - 0) < 0.1 # For Doctesting purposes\ntrue\n\n\nLet's try some other options\n\njulia> using Distances: Cityblock\n\njulia> parameters = CoTETE.CoTETEParameters(l_x = 1,\n                                            l_y = 2,\n                                            k_global = 3,\n                                            auto_find_start_and_num_events = false,\n                                            start_event = 100,\n                                            num_target_events = 5000,\n                                            num_samples_ratio = 2.3,\n                                            metric = Cityblock());\n\njulia> TE = CoTETE.estimate_TE_from_event_times(parameters, target, source)\n0.0\n\njulia> abs(TE - 0) < 0.1 # For Doctesting purposes\ntrue\n\nThe next example applies the estimator to a more complex problem, specifically, the process described as example B in Spinney et. al.. The application of the estimator to this example is covered in section II B of our paper. We create the source process as before. Howevever, the target process is originally created as an homogeneous Poisson process with rate 10, before a thinning algorithm is applied to it, in order to provide the dependence on the source.\n\njulia> source = sort(1e4*rand(Int(1e4)));\n\njulia> target = sort(1e4*rand(Int(1e5)));\n\njulia> function thin_target(source, target, target_rate)\n           # Remove target events occurring before first source\n    \t   start_index = 1\n    \t   while target[start_index] < source[1]\n           \t start_index += 1\n    \t   end\n    \t   target = target[start_index:end]\n\n\t   new_target = Float64[]\n    \t   index_of_last_source = 1\n    \t   for event in target\n               while index_of_last_source < length(source) && source[index_of_last_source + 1] < event\n               \t     index_of_last_source += 1\n               end\n               distance_to_last_source = event - source[index_of_last_source]\n               λ = 0.5 + 5exp(-50(distance_to_last_source - 0.5)^2) - 5exp(-50(-0.5)^2)\n               if rand() < λ/target_rate\n               \t  push!(new_target, event)\n               end\n           end\n    \t   return new_target\n       end\n\njulia> target = thin_target(source, target, 10);\n\njulia> parameters = CoTETE.CoTETEParameters(l_x = 1, l_y = 1);\n\njulia> TE = CoTETE.estimate_TE_from_event_times(parameters, target, source)\n0.5076\n\njulia> abs(TE - 0.5076) < 0.05 # For Doctesting purposes\ntrue\n\nWe can also try extending the length of the target embeddings in order to better resolve this dependency\n\njulia> parameters = CoTETE.CoTETEParameters(l_x = 3, l_y = 1);\n\njulia> TE = CoTETE.estimate_TE_from_event_times(target, source, 3, 1)\n0.5076\n\njulia> abs(TE - 0.5076) < 0.05 # For Doctesting purposes\ntrue\n\n\n\n\n\n","category":"function"},{"location":"public/#","page":"Public Interface","title":"Public Interface","text":"estimate_TE_and_p_value_from_event_times","category":"page"},{"location":"public/#CoTETE.estimate_TE_and_p_value_from_event_times","page":"Public Interface","title":"CoTETE.estimate_TE_and_p_value_from_event_times","text":"function estimate_TE_and_p_value_from_event_times(\n    parameters::CoTETEParameters,\n    target_events::Array{<:AbstractFloat},\n    source_events::Array{<:AbstractFloat};\n    conditioning_events::Array{<:AbstractFloat} = Float32[],\n    return_surrogate_TE_values::Bool = false,\n)\n\ncalculate the TE and the p value of it being statistically different from zero.\n\nThis example demonstrates estimating the TE and p value between uncoupled homogeneous Poisson processes. As the true value of the TE is zero, we expect the p value to be uniformly disributed between zero and one.\n\nWe first create the source and target processes, each with 1 000 events and with rate 1, before running the estimator and the surrogate generation procedure.\n\nExamples\n\njulia> source = sort(1e3*rand(Int(1e3)));\n\njulia> target = sort(1e3*rand(Int(1e3)));\n\njulia> parameters = CoTETE.CoTETEParameters(l_x = 1, l_y = 1);\n\njulia> TE, p = CoTETE.estimate_TE_and_p_value_from_event_times(parameters, target, source)\n(0.0, 0.5)\n\njulia> p > 0.05 # For Doctesting purposes. Should fail every now and then.\ntrue\n\n\nThis second example shows using this method on coupled processes for which the true value of the TE is nonzero. As there is a strong coupling between the source and target, we expect the p value to be close to 0. The application of the estimator to this example is covered in section II B of our paper. See the above examples for estimate_TE_from_event_times for more details as well as the implementation of the thinning algorithm.\n\nWe create the source process as before. Howevever, the target process is originally created as an homogeneous Poisson process with rate 10, before the thinning algorithm is applied to it, in order to provide the dependence on the source.\n\njulia> source = sort(1e3*rand(Int(1e3)));\n\njulia> target = sort(1e3*rand(Int(1e4)));\n\njulia> target = thin_target(source, target, 10);\n\njulia> parameters = CoTETE.CoTETEParameters(l_x = 1, l_y = 1);\n\njulia> TE, p = CoTETE.estimate_TE_and_p_value_from_event_times(parameters, target, source)\n(0.5, 0.01)\n\njulia> p < 0.05 # For Doctesting purposes. Should fail very rarely.\ntrue\n\n\n\n\n\n","category":"function"},{"location":"public/#","page":"Public Interface","title":"Public Interface","text":"estimate_AIS_from_event_times","category":"page"},{"location":"public/#CoTETE.estimate_AIS_from_event_times","page":"Public Interface","title":"CoTETE.estimate_AIS_from_event_times","text":"estimate_AIS_from_event_times(\n  parameters::CoTETEParameters,\n  target_events::Array{<:AbstractFloat},\n  source_events::Array{<:AbstractFloat};\n  conditioning_events::Array{<:AbstractFloat} = Float32[],\n)\n\nEstimate the Active Information Storage (AIS) from lists of raw event times.\n\nSee this thesis for a description of AIS.\n\nExamples\n\nThis example estimates the AIS on an homogeneous Poisson process. The true value of the AIS on such a process is zero.\n\njulia> target = sort(1e4*rand(Int(1e4)));\n\njulia> parameters = CoTETE.CoTETEParameters(l_x = 1);\n\njulia> AIS = CoTETE.estimate_AIS_from_event_times(parameters, target)\n0.0\n\njulia> abs(AIS - 0) < 0.05 # For Doctesting purposes\ntrue\n\n\n\n\n\n\n","category":"function"},{"location":"public/#","page":"Public Interface","title":"Public Interface","text":"estimate_AIS_and_p_value_from_event_times","category":"page"},{"location":"public/#CoTETE.estimate_AIS_and_p_value_from_event_times","page":"Public Interface","title":"CoTETE.estimate_AIS_and_p_value_from_event_times","text":"estimate_AIS_and_p_value_from_event_times(\n  parameters::CoTETEParameters,\n  target_events::Array{<:AbstractFloat},\n  source_events::Array{<:AbstractFloat};\n  conditioning_events::Array{<:AbstractFloat} = Float32[],\n)\n\nEstimate the Active Information Storage (AIS) along with the p value of the AIS being different from 0 from lists of raw event times.\n\nSee this thesis for a description of AIS.\n\nExamples\n\nThis example estimates the AIS and p value on an homogeneous Poisson process. The true value of the AIS on such a process is zero. We expect the p value to be uniformly distributed between zero and 1.\n\njulia> target = sort(1e3*rand(Int(1e3)));\n\njulia> parameters = CoTETE.CoTETEParameters(l_x = 1);\n\njulia> AIS, p = CoTETE.estimate_AIS_and_p_value_from_event_times(parameters, target)\n(0.0, 0.5)\n\njulia> p > 0.05 # For Doctesting purposes. Should fail from time to time\ntrue\n\n\nThis next example estimates the AIS for a process where we know that the AIS must be nonzero. This process has an event occurring every one time unit, with a bit of noise added to the event times.\n\njulia> target = sort(cumsum(ones(Int(1e3))) .+ 1e-2*randn(Int(1e3)));\n\njulia> parameters = CoTETE.CoTETEParameters(l_x = 1);\n\njulia> AIS, p = CoTETE.estimate_AIS_and_p_value_from_event_times(parameters, target)\n(1.0, 0.01)\n\njulia> p < 0.05 # For Doctesting purposes. Should fail from time to time\ntrue\n\n\n\n\n\n\n","category":"function"},{"location":"public/#","page":"Public Interface","title":"Public Interface","text":"CoTETEParameters","category":"page"},{"location":"public/#CoTETE.CoTETEParameters","page":"Public Interface","title":"CoTETE.CoTETEParameters","text":"struct CoTETEParameters\n    l_x::Integer = 0\n    l_y::Integer = 0\n    l_z::Integer = 0\n    auto_find_start_and_num_events::Bool = true\n    num_target_events_cap::Integer = -1\n    start_event::Integer = 1\n    num_target_events::Integer = 0\n    num_samples_ratio::AbstractFloat = 1.0\n    k_global::Integer = 5\n    metric::Metric = Euclidean()\n    kraskov_noise_level::AbstractFloat = 1e-8\n    num_surrogates::Integer = 100\n    surrogate_num_samples_ratio::AbstractFloat = 1.0\n    k_perm::Integer = 5\nend\n\nl_x::Integer: The number of intervals in the target process to use in the history embeddings. Corresponds to l_X in [1].\nl_y::Integer: The number of intervals in the source process to use in the history embeddings. Corresponds to l_Y in [1].\nl_z::Integer = 0: The number of intervals in the single conditioning process to use in the history embeddings. Corresponds to l_Z_1 in [1].\ninfo: Single conditioning process\nNote that although the framework developed in our paper considers an arbitrary number of extra conditioning processes, at present the framework can only handle a single such process. This will change in future releases.\nauto_find_start_and_num_events::Bool = true: When set to true, the start event will be set to the first event for which there are sufficient preceding events in all processes such that the embeddings can be constructed. The number of target events will be set such that all time between this first event and the last target event is included.\nnum_target_events_cap::Integer = -1\nstart_event::Integer = 1: only used when auto_find_start_and_num_events = false. The index of the event in the target process from which to start the analysis.\nnum_target_events::Integer = 0: only used when auto_find_start_and_num_events = false. The TE will be calculated on the time series from the timestamp of the start_event-th event of the target process to the timestamp of the start_event + num_target_events-th event of the target process.\nnum_samples_ratio::AbstractFloat = 1.0: Controls the number of samples used to estimate the probability density of histories unconditional of the occurrence of events in the target process. This number of samples will be num_samples_ratio * num_target_events. Corresponds to N_UN_X in [1].\nk_global::Integer = 5: The number of nearest neighbours to consider in initial searches.\nmetric::Metric = Euclidean(): The metric to use for nearest neighbour and radius searches.\nnum_surrogates::Integer = 100:\nsurrogate_num_samples_ratio::AbstractFloat = 1.0: Controls the number of samples used to to construct the alternate set of history embeddings used by our local permutation scheme. This number of samples will be surrogate_num_samples_ratio * num_target_events. Corresponds to N_U textrmsurrogateN_X in [1].\nk_perm::Integer = 5: The number of neighbouring source embeddings from which to randomly select a replacement embedding in the local permutation scheme.\n\n[1] Shorten, D. P., Spinney, R. E., Lizier, J.T. (2020). Estimating Transfer Entropy in Continuous Time Between Neural Spike Trains or Other Event-Based Data. bioRxiv 2020.06.16.154377.\n\n[2] Spinney, R. E., Prokopenko, M., & Lizier, J. T. (2017). Transfer entropy in continuous time, with applications to jump and neural spiking processes. Physical Review E, 95(3), 032319.\n\n\n\n\n\n","category":"type"},{"location":"background/#Background-1","page":"Background","title":"Background","text":"","category":"section"},{"location":"background/#","page":"Background","title":"Background","text":"Transfer entropy is a measure of information flow between a source and a target time series. In the context of event-based data, it measures how much the knowledge of the times of historic events in the source decreases our uncertainty about the occurrence of events in the target. Getting a clearer picture of what TE is measuring is easiest done, initially at least, in discrete time.","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"(Image: Discrete TE)","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"The above diagram shows the raw membrane potentials of two neurons from which spikes are extracted. Time is then discretised into bins of width Delta t to give us two binary time series (labelled Y and X for the source and target, respectively). The binary values these processes take on signify the presence of an event (spike) in each bin. For each such value x_t in the target process, we can ask what the probability of that value is given the history of the target process p(x_t    mathbfx_t). In practice, such conditional probabilities can only be estimated for histories of limited length. As such, we use a history embedding of m bins. This embedding is usually chosen to be the m bins preceding the t-th bin under consideration. In the above diagram, m is chosen to be 4, and so we are estimating p(x_t    mathbfx_t-4t-1). So, for the specific example x_t pulled out of the diagram, we are asking what the probability of the bin having a value of 0 is (that is, the probability of there being no spike in the bin), given that the preceding 4 bins were 0 1 0 0.","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"We can also ask what the probability of x_t is given history embeddings of both the source and the target: p(x_t    mathbfx_t-4t-1 mathbfy_t-4t-1). Looking at our specific pulled-out example again, we would be asking for the probability of the target bin having a value of 0 given that the preceding 4 bins of the target were 0 1 0 0 and the preceding 4 bins of the source were 0 1 0 1.","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"We can then compare the probabilities p(x_t    mathbfx_t-4t-1 mathbfy_t-4t-1) and p(x_t    mathbfx_t-4t-1) to determine whether the knowledge of the source reduced our uncertainty about the next state of the target. If p(x_t    mathbfx_t-4t-1 mathbfy_t-4t-1)  p(x_t    mathbfx_t-4t-1) then the source allowed us to better predict the next state of the target and so reduced our uncertainty. Conversely, if p(x_t    mathbfx_t-4t-1 mathbfy_t-4t-1)  p(x_t    mathbfx_t-4t-1) then the source was misinformative.","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"One way of turning these two probabilities into a measurement of \"informativeness\" is to take the log of their ratio. We shall label this mathbft_Y to X.","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":" mathbft_Y to X = lnfracp(x_t    mathbfx_t mathbfy_t)p(x_t    mathbfx_t)","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"We use mathbft_Y to X^l m for the same quantity calculated for specific source and target history embedding lengths.","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":" mathbft_Y to X^l m = lnfracp(x_t    mathbfx_t-mt-1 mathbfy_t-lt-1)p(x_t    mathbfx_t-mt-1)","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"mathbft_Y to X will be positive in cases where the source is informative and negative when it is misinformative. However, it is only a measure of how informative the source was in that specific time bin. In many cases, such as network inference, we are interested in how generally informative one time series is of another. As such, we take the average of mathbft_Y to X and label it mathbfT_Y to X.","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"mathbfT_Y to X = frac1Nsum_t=1^N\nlnfrac\n  p(x_t    mathbfx_t mathbfy_t)\n  \n    p(x_t    mathbfx_t)\n  ","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"The average here is taken over the N time bins of the target series. mathbfT_Y to X is the transfer entropy. In order to ensure that we have a quantity that converges in the limit of small bin size, we usually normalise the TE by the bin size Delta t, to arrive at the transfer entropy rate mathbfdotT_Y to X = frac1Delta tmathbfT_Y to X. This rate is the normalised conditional mutual information between the  current time bin of the target and the history of the source, conditioned on the history of the target. That is:","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"mathbfdotT_Yrightarrow X\n\t=\n\tfrac1Delta t\n\tIleft(\n\t\tX_t    mathbfY_t\n\t\t middle \n\t\tmathbfX_t\n\tright)","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"Due to the fact that the mutual information is non-negative, the transfer entropy rate is similarly non-negative, despite the fact that the individual mathbft_Y to X terms that make up the average can be negative.","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"The estimation of mathbfdotT_Y to X on event-based data has traditionally been done in a very straightforward fashion. After performing time discretisation, for each time bin t, we estimate the probability of the value in that bin given its history embedding, p(x_t    mathbfx_t-mt-1) using a plug-in or histogram estimator. Specifically, we find every occurrence of the embedding mathbfx_t-mt-1 and record the values in the subsequent bin. The estimated probability is then the number of times we observed the same value as x_t divided by the total number of values recorded. Going back to our example, for the specific bin pulled out, we would find all instances of the pattern 0 1 0 0 in the discretised time series. We would then count how many of these instances were followed by a 0. The estimated conditional probability is then the ratio of the number of instances follwed by a 0 to the total number of instances. p(x_t    mathbfx_t-mt-1 mathbfy_t-lt-1) is estimated in a similar fashion, except we are now looking for instances where both mathbfx_t-mt-1 and mathbfy_t-lt-1 match. We take the log of the ratio of these estimated probabilities to provide an estimate for mathbft_Y to X. We then repeat this procedure for every time bin, find the average, normalise by Delta t and, wham, we have estimated the TE!! (There are more computationally efficient ways of arriving at the same quantity, but we are not \tconcerned with efficiency right now)","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"Unfortunately, there are large limitations to estimating the TE in this fashion. Perhaps the most important of these is that when we perform time-discretisation, we are applying a lossy transformation to the data. So long as our bin size is larger than the time precision of the device used to record our data (as is usually the case), once the discretisation has been performed we cannot reconstruct the original time series in full detail - we have lost information. There could be history dependencies in the original data which occurr over a time scale smaller than the bin size. The discrete-time estimator will be blind to these relationships as the fine time detail was lost during discretisation. The implication of this is that the discrete-time estimator is not guaranteed to converge to the true value of the TE in the limit of infinite data (it is not consistent).","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"Another large limitation relates to our choice of bin size Delta t. The number of different history permutations grows exponentially in the number of bins that we use. On most computers this will limit us to a total budget of 20 to 25 bins accross all history embeddings before we run out of memory. Further, this exponential increase in the number of history permutations means that if we use too many bins in each embedding, then we risk undersampling the individual permutations. If a given history permutation only occurrs once or twice we cannot get an accurate estimate of the probability of the subsequent bin having a spike. The resulting limited budget on the bin-length of our history embeddings means that our choice of bin size will lead to a hard tradeoff. If we want to capture the influence of events occurring far in the past we can use large bins. On the other hand, if we want to capture relationships that occurr with fine time precision we can use smaller bins. However, we cannot capture both simultaneously.","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"The discrete-time estimator also provides estimates that are systematically far from the true value when it is provided with smaller datasets (it has high bias). This problem is compounded by the fact that its convergence to the true value is slow (see our paper for examples).","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"CoTETE.jl presents an implementation of an estimator that is able to bypass these issues. It operates in continuous time on the raw timestamps of the events. This allows it to be consistent –- it is guaranteed to converge to the true value of the TE rate in the limit of large dataset size. The fact that it uses inter-event intervals for its history embeddings allows it to capture dependencies over relatively long ranges without any loss of precision. It also exhibits bias and convergence properties that are far superior to the discrete-time approach (see our paper for examples).","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"The below figure gives a diagram of how the history embeddings are represented in our approach.","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"(Image: Continuous TE)","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"The estimator makes use of a recent result which showed that, for stationary event-based data, the TE rate can be expressed as:","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"mathbfdotT_Yrightarrow X\n=\nlim_tau to infty\nfrac1tau\nsum_i=1^N_X\nln\nfrac\n\tlambda_\n\t\txmathbfx_tmathbfy_t\n\t\n\tleft\n\tmathbfx_x_imathbfy_x_i\n\tright\n\t\n\t\tlambda_\n\t\t\txmathbfx_t\n\t\t\n\t\tleft\n\t\tmathbfx_x_i\n\t\tright\n\t","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"To avoid confusion, it is worth emphasizing that the structure of the underlying data we are analysing has changed here. In the discrete-time case, X and Y were series of values at the sampled time points t_i. x_t in X was then a binary value representing the presence or absence of a spike in the t-th bin. Here, however, X and Y are sets of the raw timestamps of the events. x_i in X is then the time at which a spike in the target occurred. To avoid confusion around the history embeddings, we denote by mathbfy_x_i some representation of the history of Y observed at the time point x_i. lambda_ \t\txmathbfx_tmathbfy_t \t \tleft \t\tmathbfx_x_imathbfy_x_i \tright is the instantaneous firing rate (rate of the occurrence of events per unit time) of the target conditioned on the histories of the target mathbfx_x_i and source mathbfy_x_i at the time points x_i of the events in the target process. lambda_ \t\txmathbfx_t \t \tleft \t\tmathbfx_x_i \tright is the instantaneous firing rate of the of the target conditioned on its history alone, ignoring the history of the source. Note that lambda_xmathbfx_tmathbfy_t and lambda_xmathbfx_t are defined at all points in time and not only at target events. tau is the lenght of the target process.","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"An important property of this expression is that the sum is taken over the N_X events in the target process. No contributions need to be estimated for the 'white space' in-between events. It was shown that the discrete-time expression of the TE rate converges to this expression in the limit of small bin size. Fortunately, the contributions between events cancel and we are left with an expression which can be used for efficient estimation of the TE rate.","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"The dominant method for the estimation of information-theoretic quantities from continuous-valued data is the class of k-Nearest-Neighbours (kNN) estimators. There are multiple consistency proofs for the various estimators in this class, so we have guarantees that we will converge to the correct answer in the limit of infinite data.","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"Gaining an understanding of how these kNN estimators operate is most easily done in the simplest case of estimating the differential entropy of a random variable. Say we have the variable mathbfZ, the differential entropy is then H(mathbfZ) = -mathbbE_P(mathbfZ)ln p(mathbfz). Here, mathbbE_P(mathbfZ) represents that we are taking the expected value or average. We are attempting to estimate the entropy from a set of N_Z samples drawn from the distribution P(mathbfZ). The below diagram shows an example of such a set of samples in an instance where mathbfz is two dimensional (mathbfz = z_1 z_2). Each sample in the set is represented by a point in the below diagram.","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"(Image: knn)","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"In real applications, the underlying distribution P(mathbfZ) is unkown (if it were known we probably wouldn't need the estimator), but we have observed the set of samples from it in some experiment or data collection. Our strategy for estimating the entropy is to go through each of the sample points mathbfz_i and find an estimate of the probability density at this point, hatp(mathbfz_i). We can then take the negative log of these estimates and average them to come up with an estimator for the entropy:","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"hatH(mathbfZ) = -frac1N_Zsum_i=1^N_Z ln hatp(mathbfz_i)","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"We now just need to find a way to construct the estimator for the probability density hatp(mathbfz_i) and this is where the k-Nearest-Neighbours searches are used. For a given point mathbfz_i, we search for the k-th closest point to mathbfz_i according to a distance metric of our choice. We record the distance epsilon to this point. The probability density can then be estimated as the ratio of the probability mass k(N_Z - 1) to the volume of the epsilon-ball formed around the point. This process is demonstrated in the above figure, for a two-dimensional variable with k = 3 and the euclidean distance. For p-norms we can express this volume as c_d L^pepsilon^d where c_d L^p is the volume of the d-dimensional unit ball under the norm L^p. The exact expression for c_d L^p is not very important as it will end up cancelling out later. We will usually use the Manhattan, maximum or Euclidean norm.","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"This gives us:","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"hatp(mathbfz_i)\n=\nfrac\n\tk\n\t\n\t\tleft(N_Z - 1right)\n\t\tc_d L\n\t\tepsilon_i^d\n\t","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"Integrating this into our strategy for estimating entropy, we have the estimator:","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"hatH(Z) =\n-frac1N_Z sum_i=1^N_Z\nln\nfrac\n\tk\n\t\n\t\tleft(N_Z - 1right)\n\t\tc_d L\n\t\tepsilon_i^d\n\t","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"We then add the bias-correction term ln k - psi(k). psi(x) is the digamma function. This gives us hatH_textKL, the Kozachenko-Leonenko estimator of differential entropy:","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"\t\thatH_textKL(Z) = -psi(k) + ln(N_Z - 1) + ln c_d L  \n\t\t+ fracdN_Z sum_i=1^N_Z\n\t\tln epsilon_i","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"It is most unfortunate that Kozachenko-Leonenko and Kullback-Leibler share the same abbreviation of their surnames. This has caught me on a few occassions.","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"Before we can move on, however, we need one more arrow in our quiver: a kNN estimator of cross entropy. The differential cross entropy can be written as H_A(mathbfZ) = -mathbbE_P(mathbfZ)ln p_A(mathbfz). Here the probability density to which we apply the logarithm is different to the distribution over which we take the expectation. This actually starts to make a bit more intuitive sense when we start to think about how we would estimate the this quantity using a  k-NN estimator. We have our set of sample points drawn from the distribution P(mathbfZ). We then draw another set of sample points from the distribution P_A(mathbfZ). We go through each point in the first set (because the expectation is over P(mathbfZ)), and at each point find the k nearest neighbours in the second set (because we want the density p_A(mathbfz)). We can then use the same expression that we did for the KL estimator of the entropy. The below diagram illustrates this process. The blue dots represent the first set of samples drawn from P(mathbfZ). The red crosses represent the second set of samples drawn from P_A(mathbfZ).","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"(Image: knn_cross)","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"k-NN estimators of other information-theoretic quantities (such as mutual information and KL divergence)  operate by decomposing the quantity into a sum of entropy and cross-entropy terms. Each of these terms can then be estimated using hatH_textKL. Sometimes, as in the case of the famous KSG estimator of mutual information, a scheme is divised whereby the same radius is used for a given point across multiple entropy terms. This has been found to reduce the bias.","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"Unfortunately, if we try to apply this strategy to our expression for the TE rate we will hit a snag. This is because this expression is written in terms of logs of rates, as opposed to logs of probability densities. This means that we have no entropy terms!!","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"The situation can be remedied by re-writing our expression for the TE in continuous time as follows:","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"mathbfdotT_Y rightarrow X =\n\tbarlambda_X\n\t\tmathbbE_P_X\n\t\tbigg\n\t\t\t\tln\n\t\tfrac\n\t\t\tp_X left(\n\t\t\t\tmathbfx_x mathbfy_x\n\t\t\tright)\n\t\t\n\t\t\tp_X left(\n\t\t\t\tmathbfx_x\n\t\t\tright)\n\t\t\n\t\t+ ln\n\t\tfrac\n\t\t\tp_U left(\n\t\t\t\tmathbfx_x\n\t\t\tright)\n\t\t\n\t\t\tp_U left(\n\t\t\t\tmathbfx_x mathbfy_x\n\t\t\tright)\n\t\t\n\t\tbigg","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"See our paper for a derivation. As this expression has probability densities, as opposed to rates, it will yield entropy and cross-entropy terms. The densities p_X refer to the probability  density of histories observed at events in the target process and the densities p_U refer to the probability densities of histories observed anywhere in the time series, not conditioned on events in the target process. barlambda_X is the average rate of events in the target process.","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"This expression has shifted our view on the probabilities that we are trying to estimate. In the original continuous-time expression for the TE rate, we were asking, given a certain history, what the probability of an event is in the next small time window. In this new expression we are asking, given a certain history observed at an event, what is the probability density of observing that history at an event. We contrast this with the probability of observing that history anywhere in the process.   ","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"Writing our new expression as a sum of entropy and cross entropy terms we have:","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"\t\tdotmathbfT_Y rightarrow X =\n\t\tbarlambda_X\n\t\tbigg\n\t\t- Hleft(\n\t\t\tmathbfX_X mathbfY_X\n\t\tright)\n\t\t+ Hleft(\n\t\t\tmathbfX_X\n\t\tright)\n\t\t+ H_P_Uleft(\n\t\t\tmathbfX_X mathbfY_X\n\t\tright)\n       - H_P_Uleft(\n\t\t\tmathbfX_X\n\t\tright)\n\t\tbigg","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"The only outstanding issue that needs addressing is how we draw samples according to P_X and P_U. As P_X is just the distribution of histories observed at events in the target process, we just construct history embeddings at each target event and this is our set of samples. For P_U we construct history embeddings at random points in the process.","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"We now just apply the KL estimators of differential entropy and cross entropy and we have an estimate of the TE rate!!","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"CoTETE.jl actually operates in a slightly more complicated fashion, utilising a scheme to share radii across entropy terms for a given point. See our paper section IV A 5 for the details of this scheme.","category":"page"},{"location":"quickStart/#Quick-Start-1","page":"Quick Start","title":"Quick Start","text":"","category":"section"},{"location":"quickStart/#","page":"Quick Start","title":"Quick Start","text":"Install Julia","category":"page"},{"location":"quickStart/#","page":"Quick Start","title":"Quick Start","text":"Clone this repo (make sure to include the –recurse-submodules flag so that the modified nearest neighbours package gets included).","category":"page"},{"location":"quickStart/#","page":"Quick Start","title":"Quick Start","text":"david@home:~$ git clone --recurse-submodules https://github.com/dpshorten/CoTETE.jl.git","category":"page"},{"location":"quickStart/#","page":"Quick Start","title":"Quick Start","text":"make sure that CoTETE.jl/src/ is on your JULIA_LOAD_PATH. eg:","category":"page"},{"location":"quickStart/#","page":"Quick Start","title":"Quick Start","text":"david@home:~$ export JULIA_LOAD_PATH=:/home/david/CoTETE.jl/src/","category":"page"},{"location":"quickStart/#","page":"Quick Start","title":"Quick Start","text":"Fire up the Julia REPL","category":"page"},{"location":"quickStart/#","page":"Quick Start","title":"Quick Start","text":"david@home:~$ julia","category":"page"},{"location":"quickStart/#","page":"Quick Start","title":"Quick Start","text":"You will need to add three prerequisite packages.","category":"page"},{"location":"quickStart/#","page":"Quick Start","title":"Quick Start","text":"tip: Tip for new Julia users\nThe Julia REPL has a nifty feature called prompt pasting, which means that it will automatically remove the julia> prompt when you paste. You can, therefore, just copy and paste the entire block below without worrying about these prompts.","category":"page"},{"location":"quickStart/#","page":"Quick Start","title":"Quick Start","text":"julia> import Pkg\njulia> Pkg.add(\"Distances\")\njulia> Pkg.add(\"StaticArrays\")\njulia> Pkg.add(\"SpecialFunctions\")\njulia> Pkg.add(\"Parameters\")","category":"page"},{"location":"quickStart/#","page":"Quick Start","title":"Quick Start","text":"For the first example, lets estimate the TE between uncoupled homogeneous Poisson processes. This is covered in section II A of [1]. We first create the source and target processes, each with 10 000 events and with rate 1.","category":"page"},{"location":"quickStart/#","page":"Quick Start","title":"Quick Start","text":"julia> source = 1e3*rand(Int(1e3));\njulia> sort!(source);\njulia> target = 1e3*rand(Int(1e3));\njulia> sort!(target);","category":"page"},{"location":"quickStart/#","page":"Quick Start","title":"Quick Start","text":"We can now estimate the TE between these processes, with history embeddings of length 1.","category":"page"},{"location":"quickStart/#","page":"Quick Start","title":"Quick Start","text":"julia> import CoTETE\njulia> CoTETE.estimate_TE_from_event_times(target, source, 1, 1)","category":"page"},{"location":"quickStart/#","page":"Quick Start","title":"Quick Start","text":"The answer should be close to 0.","category":"page"},{"location":"#CoTETE.jl-1","page":"Home","title":"CoTETE.jl","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Continuous-Time Event-based Transfer Entropy","category":"page"},{"location":"#","page":"Home","title":"Home","text":"CurrentModule = CoTETE","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Transfer entropy (TE) is a measure of information flow between time series. It can be used to infer functional networks of statistical associations. Under certain assumptions it can also be used to estimate underlying causal networks from observational data.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"This package allows one to estimate the TE between event-based time series (such as spike trains or social media post times) in continuous time (that is, without discretising time into bins). The advantages of this approach over the discrete-time approach include:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The continuous-time approach is provably consistent - it is guaranteed to converge to the true value of the TE in the limit of infinite data. The discrete-time estimator is not consistent. It is easy to create examples where it does not converge to the true value of the TE.\nThe discrete-time approach is thwarted by having an effective limit on the total number of bins that can be used for history embeddings. This means that the user of this approach must choose between capturing relationships occurring over long time intervals, or those that occurr with fine time precision (by choosing either a large or small bin size Delta t). They can never capture both simultaneously. By contrast, the continuous-time approach can capture relationships occurring over relatively long time intervals with no loss of precision.\nOn synthetic examples studied, the continuous-time approach converges orders of magnitude faster than the discrete-time approach and exhibits substantially lower bias.\nIn the inference of structural and functional connectivity, the discrete-time approach was typically coupled with a surrogate generation method which utilised an incorrect null hypothesis. The use of this method can be demonstrated to lead to high false-positive rates. CoTETE.jl contains an implementation of a method for generating surrogates which conform to the correct null hypothesis of conditional independence.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"See our paper for more details on all of these points.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Transfer entropy has already been widely applied to the spiking activity of neurons. Notable work on the application of TE to spike trains include:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The reconstruction of the structural connectivity of neurons from simulated calcium imaging data. See here for an extension to this work.\nThe inference of structural connectivity from models of spiking neural networks (1, 2).\nInvestigation of the energy efficiency of synaptic information transfer.\nThe inference of functional and effective networks ( 1, 2, 3, 4 )","category":"page"},{"location":"#","page":"Home","title":"Home","text":"CoTETE.jl contains implementations of the estimator and local permutation scheme presented in Estimating Transfer Entropy in Continuous Time Between Neural Spike Trains or Other Event-Based Data. If you are new to information-theoretic estimators and would like to gain an understanding of how this estimator works, I would recommend starting with the Background section of this documentation.","category":"page"},{"location":"#Contents-1","page":"Home","title":"Contents","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Pages = [\"background.md\", \"quickStart.md\", \"background\", \"public.md\", \"internals.md\"]\nDepth = 1","category":"page"},{"location":"#Other-Software-1","page":"Home","title":"Other Software","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"If you would like to apply TE to other data modalities, the JIDT toolkit is highly recommended.","category":"page"},{"location":"#Acknowledgements-1","page":"Home","title":"Acknowledgements","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"The estimator implemented here was developed in collaboration with my PhD supervisor, Joseph Lizier, as well as Richard Spinney.","category":"page"},{"location":"internals/#Internal-1","page":"Internal","title":"Internal","text":"","category":"section"},{"location":"internals/#","page":"Internal","title":"Internal","text":"CurrentModule = CoTETE","category":"page"},{"location":"internals/#","page":"Internal","title":"Internal","text":"estimate_TE_from_preprocessed_data","category":"page"},{"location":"internals/#CoTETE.estimate_TE_from_preprocessed_data","page":"Internal","title":"CoTETE.estimate_TE_from_preprocessed_data","text":"estimate_TE_from_preprocessed_data(parameters::CoTETEParameters, preprocessed_data::PreprocessedData)\n\ncalculates the TE using the preprocessed data and the given parameters.\n\njulia> source = sort(1e4*rand(Int(1e4)));\n\njulia> target = sort(1e4*rand(Int(1e4)));\n\njulia> parameters = CoTETE.CoTETEParameters(l_x = 1, l_y = 1);\n\njulia> preprocessed_data = CoTETE.preprocess_event_times(parameters, target, source_events = source);\n\njulia> TE = CoTETE.estimate_TE_from_preprocessed_data(parameters, preprocessed_data)\n0.0\n\njulia> abs(TE - 0) < 0.05 # For Doctesting purposes\ntrue\n\n\n\n\n\n\n","category":"function"},{"location":"internals/#","page":"Internal","title":"Internal","text":"make_surrogate!","category":"page"},{"location":"internals/#CoTETE.make_surrogate!","page":"Internal","title":"CoTETE.make_surrogate!","text":"function make_surrogate!(\n    parameters::CoTETEParameters,\n    preprocessed_data::PreprocessedData,\n    target_events::Array{<:AbstractFloat},\n    source_events::Array{<:AbstractFloat};\n    conditioning_events::Array{<:AbstractFloat} = Float32[],\n)\n\nEdit the source component of preprocessed_data.representation_joint such that it conforms to the null hypothesis of conditional independence.\n\n\n\n\n\n","category":"function"},{"location":"internals/#","page":"Internal","title":"Internal","text":"make_AIS_surrogate!","category":"page"},{"location":"internals/#CoTETE.make_AIS_surrogate!","page":"Internal","title":"CoTETE.make_AIS_surrogate!","text":"function make_AIS_surrogate!(\n    parameters::CoTETEParameters,\n    preprocessed_data::PreprocessedData,\n    target_events::Array{<:AbstractFloat},\n)\n\nEdit preprocessed_data.representation_joint such that it conforms to the null hypothesis of the target histories being independent of events in the target process.\n\n\n\n\n\n","category":"function"},{"location":"internals/#","page":"Internal","title":"Internal","text":"preprocess_event_times","category":"page"},{"location":"internals/#CoTETE.preprocess_event_times","page":"Internal","title":"CoTETE.preprocess_event_times","text":"function preprocess_event_times(\n    parameters::CoTETEParameters,\n    target_events::Array{<:AbstractFloat};\n    source_events::Array{<:AbstractFloat} = Float32[],\n    conditioning_events::Array{<:AbstractFloat} = Float32[]\n)\n\nUse the raw event times to create the history embeddings and other prerequisites for estimating the TE.\n\njulia> parameters = CoTETE.CoTETEParameters(l_x = 1, l_y = 1);\n\njulia> source = cumsum(ones(5)) .- 0.5; # source is {0.5, 1.5, 2.5, ...}\n\njulia> target = cumsum(ones(5)); # target is {1, 2, 3, ...}\n\njulia> preprocessed_data = CoTETE.preprocess_event_times(parameters, target, source_events = source);\n\njulia> println(preprocessed_data.representation_joint) # All target events will be one unit back, all source events 0.5 units\n[1.0 1.0 1.0 1.0; 0.5 0.5 0.5 0.5]\n\n\n\n\n\n\n","category":"function"},{"location":"internals/#","page":"Internal","title":"Internal","text":"make_embeddings_along_observation_time_points","category":"page"},{"location":"internals/#CoTETE.make_embeddings_along_observation_time_points","page":"Internal","title":"CoTETE.make_embeddings_along_observation_time_points","text":"function make_embeddings_along_observation_time_points(\n    observation_time_points::Array{<:AbstractFloat},\n    start_observation_time_point::Integer,\n    num_observation_time_points_to_use::Integer,\n    event_time_arrays::Array{<:Array{<:AbstractFloat,1},1},\n    embedding_lengths::Array{<:Integer},\n)\n\nConstructs a set of embeddings from a set of observation points. The observation points and the raw event times are assumed to be sorted. Also returns the exlcusion windows.\n\nExample\n\njulia> source = cumsum(ones(20)) .- 0.5; # source is {0.5, 1.5, 2.5, ...}\n\njulia> conditional = cumsum(ones(20)) .- 0.25; # conditional is {0.75, 1.75, 2.75, ...}\n\njulia> target = cumsum(ones(20)); # target is {1, 2, 3, ...}\n\njulia> observation_points = cumsum(ones(20)) .- 0.75; # observation points are {0.25, 1.25, 2.25, ...}\n\njulia> CoTETE.make_embeddings_along_observation_time_points(observation_points, 5, 2, [target, source, conditional], [2, 1, 1])\n([0.25 0.25 0.25; 1.0 1.0 1.0; 0.75 0.75 0.75; 0.5 0.5 0.5], [3.0 4.25]\n\n[4.0 5.25]\n\n[5.0 6.25])\n\n\n\n\n\n\n","category":"function"},{"location":"internals/#","page":"Internal","title":"Internal","text":" make_one_embedding","category":"page"},{"location":"internals/#CoTETE.make_one_embedding","page":"Internal","title":"CoTETE.make_one_embedding","text":"function make_one_embedding(\n    observation_time_point::AbstractFloat,\n    event_time_arrays::Array{<:Array{<:AbstractFloat, 1}, 1},\n    most_recent_event_indices::Array{<:Integer},\n    embedding_lengths::Array{<:Integer},\n)\n\nConstructs the history embedding from a given point in time. Also returns the timestamp of the earliest event used in the construction of the embedding. This is used for recording the exclusion windows.\n\nExample\n\njulia> source = cumsum(ones(20)) .- 0.5; # source is {0.5, 1.5, 2.5, ...}\n\njulia> conditional = cumsum(ones(20)) .- 0.25; # conditional is {0.75, 1.75, 2.75, ...}\n\njulia> target = cumsum(ones(20)); # target is {1, 2, 3, ...}\n\njulia> CoTETE.make_one_embedding(5.25, [target, source, conditional], [5, 5, 5], [2, 3, 1])\n(Any[0.25, 1.0, 0.75, 1.0, 1.0, 0.5], 2.5)\n\n\n\n\n\n","category":"function"},{"location":"internals/#","page":"Internal","title":"Internal","text":"PreprocessedData","category":"page"},{"location":"internals/#CoTETE.PreprocessedData","page":"Internal","title":"CoTETE.PreprocessedData","text":"representation_joint::Array{<:AbstractFloat, 2}\nrepresentation_conditionals::Array{<:AbstractFloat, 2}\nexclusion_windows::Array{<:AbstractFloat, 3}\nsampled_representation_joint::Array{<:AbstractFloat, 2}\nsampled_representation_conditionals::Array{<:AbstractFloat, 2}\nsampled_exclusion_windows::Array{<:AbstractFloat, 3}\nstart_timestamp::AbstractFloat\nend_timestamp::AbstractFloat\n\nThe transformed data that is fed into the search trees.\n\nrepresentation_joint::Array{<:AbstractFloat, 2}: Contains the history representation of the source, target and extra conditioning process at each target event. Has dimension (l_X + l_Z_1 + l_Y) times N_X. Rows 1 to l_X (inclusive) contain the components relating to the target process. Rows l_X + 1 to l_X + l_Z_1 contain the components relating to the conditioning process. Rows l_X  + l_Z_1 + 1 to l_X + l_Z_1 + l_Y contain the components relating to the source process. A similar convention is used by sampled_representation_joint. Note that we do not include an array in this struct to keep track of the history embeddings for the conditioning variables. This is because this array is simply the first l_X + l_Z_1 rows of this array and so can easily be constructed on the fly later.\nexclusion_windows::Array{<:AbstractFloat, 3}: Contains records of the time windows around each representation made at target events which must be excluded when doing kNN searches from that representation. By default, each representation has the window that is bound to the left by the first point in time that was used to make an embedding at that sample and to the right by the timestamp of the target event itself. An extra window might be added if the representation is a surrogate. In this case, the second window will be the original window of the representation with which the source component is swapped. Has dimension N_E times 2 times N_X. N_E is the number of exclusion windows the representation has (one by default, two if it is a surrogate). Note that a single set of exclusion windows is used for the representations of both the joints and the conditionals. Using separate sets of windows would allow them to be slightly smaller, but the effect will be negligible for longer processes.\nsampled_representation_joint::Array{<:AbstractFloat, 2}: Contains the history representation of the source, target and extra conditioning processes at each sample point. Has dimension (l_X + l_Y + l_Z_1) times N_U. See the description of representation_joint for a description of how the variables are split accross the dimensions.\nsampled_exclusion_windows::Array{<:AbstractFloat, 3}: Same as for the exclusion_windows, but contains the windows around the representations constructed at sample points.\nstart_timestamp::AbstractFloat: The raw timestamp of the first target event that is included in the analysis.\nend_timestamp::AbstractFloat: The raw timestamp of the last target event that is included in the analysis.\n\n\n\n\n\n","category":"type"},{"location":"quickStartPython/#Quick-Start-(Python)-1","page":"Quick Start (Python)","title":"Quick Start (Python)","text":"","category":"section"},{"location":"quickStartPython/#","page":"Quick Start (Python)","title":"Quick Start (Python)","text":"The first bit of these instructions is (mostly) a copy of the Julia instructions (for installation), so if you have already followed them then skip down to the first use of pip. Note, however, the one change of the addition of the installation of the Julia library PyCall.jl","category":"page"},{"location":"quickStartPython/#","page":"Quick Start (Python)","title":"Quick Start (Python)","text":"Install Julia","category":"page"},{"location":"quickStartPython/#","page":"Quick Start (Python)","title":"Quick Start (Python)","text":"Clone this repo (make sure to include the –recurse-submodules flag so that the modified nearest neighbours package gets included).","category":"page"},{"location":"quickStartPython/#","page":"Quick Start (Python)","title":"Quick Start (Python)","text":"david@home:~$ git clone --recurse-submodules https://github.com/dpshorten/CoTETE.jl.git","category":"page"},{"location":"quickStartPython/#","page":"Quick Start (Python)","title":"Quick Start (Python)","text":"make sure that CoTETE.jl/src/ is on your JULIA_LOAD_PATH. eg:","category":"page"},{"location":"quickStartPython/#","page":"Quick Start (Python)","title":"Quick Start (Python)","text":"david@home:~$ export JULIA_LOAD_PATH=:/home/david/CoTETE.jl/src/","category":"page"},{"location":"quickStartPython/#","page":"Quick Start (Python)","title":"Quick Start (Python)","text":"Fire up the Julia REPL","category":"page"},{"location":"quickStartPython/#","page":"Quick Start (Python)","title":"Quick Start (Python)","text":"david@home:~$ julia","category":"page"},{"location":"quickStartPython/#","page":"Quick Start (Python)","title":"Quick Start (Python)","text":"You will need to add three prerequisite packages.","category":"page"},{"location":"quickStartPython/#","page":"Quick Start (Python)","title":"Quick Start (Python)","text":"tip: Tip for new Julia users\nThe Julia REPL has a nifty feature called prompt pasting, which means that it will automatically remove the julia> prompt when you paste. You can, therefore, just copy and paste the entire block below without worrying about these prompts.","category":"page"},{"location":"quickStartPython/#","page":"Quick Start (Python)","title":"Quick Start (Python)","text":"julia> import Pkg\njulia> Pkg.add(\"Distances\")\njulia> Pkg.add(\"StaticArrays\")\njulia> Pkg.add(\"SpecialFunctions\")\njulia> Pkg.add(\"Parameters\")\njulia> Pkg.add(\"PyCall\")","category":"page"},{"location":"quickStartPython/#","page":"Quick Start (Python)","title":"Quick Start (Python)","text":"Install PyJulia via pip","category":"page"},{"location":"quickStartPython/#","page":"Quick Start (Python)","title":"Quick Start (Python)","text":"david@home:~$ pip3 install julia","category":"page"},{"location":"quickStartPython/#","page":"Quick Start (Python)","title":"Quick Start (Python)","text":"We can now estimate the TE between two uncoupled homogeneous Poisson processes (as covered in section II A of [1]). We the run the following code:","category":"page"},{"location":"quickStartPython/#","page":"Quick Start (Python)","title":"Quick Start (Python)","text":"from julia.api import Julia\njl = Julia(compiled_modules=False)\njl.eval(\"using CoTETE\")\nfrom julia import CoTETE\nparams = CoTETE.CoTETEParameters(l_x = 1, l_y = 1)\nimport numpy as np\ntarget = 1e3*np.random.rand(1000); target = np.sort(target);\nsource = 1e3*np.random.rand(1000); source = np.sort(source);\nCoTETE.estimate_TE_from_event_times(params, target, source)","category":"page"},{"location":"quickStartPython/#","page":"Quick Start (Python)","title":"Quick Start (Python)","text":"The answer should be close to 0.","category":"page"},{"location":"quickStartPython/#","page":"Quick Start (Python)","title":"Quick Start (Python)","text":"All the other CoTETE.jl functions documented elsewhere in these docs can be used in a similar manner. Just import the CoTETE package as above and call them as you would in Julia.","category":"page"}]
}
